{
  "title": "続・ブラウザ上でEmbeddingsを作る",
  "html": "<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Flealog.hateblo.jp%2Fentry%2F2023%2F05%2F31%2F133105\" title=\"TensorFlow.jsのUniversal Sentence Encoderで、ブラウザ上でEmbeddingsを作る - console.lealog();\" class=\"embed-card embed-blogcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 190px; max-width: 500px; margin: 10px 0px;\" loading=\"lazy\"></iframe><cite class=\"hatena-citation\"><a href=\"https://lealog.hateblo.jp/entry/2023/05/31/133105\">lealog.hateblo.jp</a></cite></p><p>これの続編。<br />\n<br />\n</p>\n\n<div class=\"section\">\n    <h3 id=\"あらすじ\">あらすじ</h3>\n    \n<ul>\n<li>前回の記事で、Tensorflow.jsのUSEを使って、ブラウザ上でEmbeddingsは作れた</li>\n<li>しかし精度がいまいち</li>\n</ul><p>というところで、より大きいモデルがあれば・・？それをブラウザから使えたら・・？と思い、いろいろ探してみた。</p><p>すると、Web AIっていう野心的なプロジェクトがあって、T5みたいなモデルも使えるらしいことがわかった。</p>\n\n    <blockquote>\n        <p><a href=\"https://github.com/visheratin/web-ai\">GitHub - visheratin/web-ai: Run modern deep learning models in the browser.</a></p>\n\n    </blockquote>\n<p>ので、やってみた。</p>\n\n</div>\n<div class=\"section\">\n    <h3 id=\"がしかし\">が、しかし</h3>\n    \n    <blockquote>\n        <p><a href=\"https://github.com/leader22/text-embeddings-by-webai\">https://github.com/leader22/text-embeddings-by-webai</a></p>\n\n    </blockquote>\n<p>動かせるようになるまで本体にPRを送ったりと時間はかかったけど、まあできた。</p><p>ただ、やっぱ日本語の精度はイマイチだな〜という感想に終わった。</p><p><blockquote data-conversation=\"none\" class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"ja\" dir=\"ltr\">続・これだから日本語は <a href=\"https://t.co/PdjDtdtWVb\">pic.twitter.com/PdjDtdtWVb</a></p>&mdash; りぃ / Yuji Sugiura (@leader22) <a href=\"https://twitter.com/leader22/status/1666297135132966913?ref_src=twsrc%5Etfw\">June 7, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> </p><br />\n<p>精度の問題をさておいても、</p>\n\n<ul>\n<li>デカいモデルを使うためにはめちゃめちゃローディング時間が必要</li>\n<li>結果を得るにもローディングが必要\n<ul>\n<li>Workerに逃がしても遅い</li>\n</ul></li>\n</ul><p>みたいなUXまわりとの兼ね合いも考えると、ブラウザで完結させようという路線自体が、一考の余地がありそう。<br />\nもっと何かに特化したタスクを、それ用のモデルで行う、みたいな制約がないと。</p><p>ビルトインじゃなく、もし日本語に特化したモデルがあるならそれを使ってみる？ってことも考えたけど、うまく見つけられなかった。</p>\n\n    <blockquote>\n        <p><a href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&library=onnx&language=ja&sort=downloads\">https://huggingface.co/models?pipeline_tag=feature-extraction&library=onnx&language=ja&sort=downloads</a></p>\n\n    </blockquote>\n<p>ML力が足りない。</p>\n\n</div>\n<div class=\"section\">\n    <h3 id=\"Transformersjs\">Transformers.js</h3>\n    <p>Web AIと同じで、`onnxruntime-web`に依存してML系のタスクができるライブラリは他にもあるらしく。</p><p>PythonのTransformersのJSポートがあったのでそれでも試してみた。</p>\n\n    <blockquote>\n        <p><a href=\"https://github.com/leader22/text-embeddings-by-transformers\">https://github.com/leader22/text-embeddings-by-transformers</a></p>\n\n    </blockquote>\n<p>結局のところ、どれだけ目的に沿った太ったモデルを使うかが全てって感じ。<br />\n楽をしようとpre-trainedなものを使おうとする限り、こと日本語を精度良く扱うことはできなそう。</p><p>デバッグしようにも、どこに原因があるのかわからず難しいケースも多いし。</p>\n\n</div>"
}
